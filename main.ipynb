{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1187efa",
   "metadata": {},
   "source": [
    "## **IMPORTACIONES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89ae7540",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd06d03f",
   "metadata": {},
   "source": [
    "## **API Key**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44143b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key cargada.\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if api_key:\n",
    "    print(\"API Key cargada.\")\n",
    "else:\n",
    "    print(\"No se encontró la API Key.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8bcd01",
   "metadata": {},
   "source": [
    "### **Carga de PDFs**\n",
    "- Cargamos los PDFs, separamos las páginas y le damos una fuente a cada una según al PDF que pertenece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec0bf95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_tag_pdfs(data_dir: str, paper_sources: dict):\n",
    "   \n",
    "    documents = []\n",
    "\n",
    "    for filename, source_title in paper_sources.items(): # Itera en el diccionario por cada PDF (toma el nombre del archivo y el título de la fuente) \n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        \n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"ADVERTENCIA: No se encontró el archivo {file_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            loader = PyPDFLoader(file_path) # Carga el PDF\n",
    "            pages = loader.load() # Carga las páginas del PDF\n",
    "            \n",
    "            for page in pages: # Etiqueta cada página con el título de la fuente (cada página de BERT.pdf tendrá la metadata \"source\": \"BERT\")\n",
    "                page.metadata[\"source\"] = source_title\n",
    "                \n",
    "            documents.extend(pages) # Agrega las páginas etiquetadas a la lista de documentos\n",
    "            print(f\"Cargado y etiquetado: {filename} ({len(pages)} páginas)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al cargar {file_path}: {e}\")\n",
    "            \n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce7461",
   "metadata": {},
   "source": [
    "### **Directorio y diccionario de los PDFs**\n",
    "- Creamos los `parametros` que va a necesitar la función que creamos previamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71b994a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargado y etiquetado: AIAYN.pdf (15 páginas)\n",
      "Cargado y etiquetado: BERT.pdf (16 páginas)\n",
      "Cargado y etiquetado: RAG.pdf (19 páginas)\n",
      "\n",
      "Total de páginas cargadas: 50\n",
      "Fuente de la página 1: Attention Is All You Need (Vaswani et al., 2017)\n"
     ]
    }
   ],
   "source": [
    "# Directorio de los PDFs\n",
    "DATA_FOLDER = \"./docs\" \n",
    "\n",
    "# Cada PDF y su título real (los títulos servirán para etiquetar las páginas)\n",
    "PAPERS = {\n",
    "    \"AIAYN.pdf\": \"Attention Is All You Need (Vaswani et al., 2017)\",\n",
    "    \"BERT.pdf\": \"BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\",\n",
    "    \"RAG.pdf\": \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)\"\n",
    "}\n",
    "\n",
    "# Llamamos a la funcion que creamos previamente pasandole el directorio y los PDFs\n",
    "all_documents = load_and_tag_pdfs(DATA_FOLDER, PAPERS)\n",
    "\n",
    "if all_documents:\n",
    "    print(f\"\\nTotal de páginas cargadas: {len(all_documents)}\")\n",
    "    print(f\"Fuente de la página 1: {all_documents[0].metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161ef3d8",
   "metadata": {},
   "source": [
    "## **Chunking**\n",
    "- Fragmentación del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30c54ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_documents(documents):\n",
    "    \n",
    "    # Configuración del separador\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000, # Traerá un máximo de 1000 caracteres por chunk\n",
    "        chunk_overlap=150, # Retrocederá hasta 150 caracteres para el siguiente chunk\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"] # Busca saltos de linea dobles, simples, puntos, etc. (hace que la separación sea más \"natural\" semánticamente).\n",
    "    )\n",
    "    \n",
    "    chunks = text_splitter.split_documents(documents) # Aplica el separador a los documentos cargados\n",
    "    \n",
    "    print(f\"Total de chunks creados: {len(chunks)}\")\n",
    "    \n",
    "    if chunks:\n",
    "        print(f\"Metadatos del primer chunk: {chunks[0].metadata['source']}\")\n",
    "        \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02581760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de chunks creados: 220\n",
      "Metadatos del primer chunk: Attention Is All You Need (Vaswani et al., 2017)\n"
     ]
    }
   ],
   "source": [
    "document_chunks = split_documents(all_documents) # Fragmentamos los documentos cargados llamando a la función previamente creada"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa445bbb",
   "metadata": {},
   "source": [
    "### **Embedding y Vector Store**\n",
    "- Convertimos cada chunk en un vector (Embbeding) y los guardamos en una BD (Vector Store) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2568989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo de embedding 'nomic-embed-text' conectado vía Ollama.\n",
      "¡Vector Store lista y guardada en ./chroma_db_tutor_ia!\n"
     ]
    }
   ],
   "source": [
    "# Conectamos con Ollama y utilizamos el modelo de embedding \"nomic-embed-text\"\n",
    "try:\n",
    "    embedding_model = OllamaEmbeddings(\n",
    "        model=\"nomic-embed-text\"\n",
    "    )\n",
    "    print(\"Modelo de embedding 'nomic-embed-text' conectado vía Ollama.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: No se pudo conectar a Ollama. ¿Está corriendo? {e}\")\n",
    "\n",
    "# Definimos dónde guardar la BD y cómo llamarla\n",
    "PERSIST_DIRECTORY = \"./chroma_db_tutor_ia\"\n",
    "COLLECTION_NAME = \"ia_papers_tutor\"\n",
    "\n",
    "# Creamos la BD\n",
    "vector_store = Chroma.from_documents(\n",
    "    documents=document_chunks, # La lista de chunks que creamos en la celda anterior\n",
    "    embedding=embedding_model,  # El modelo que usará para convertir chunks a vectores\n",
    "    persist_directory=PERSIST_DIRECTORY, # Dónde guardar la base de datos\n",
    "    collection_name=COLLECTION_NAME    # El nombre de la \"tabla\" dentro de la BD\n",
    ")\n",
    "\n",
    "print(f\"¡Vector Store lista y guardada en {PERSIST_DIRECTORY}!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d31139",
   "metadata": {},
   "source": [
    "---\n",
    "## **PIPELINE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adbde35",
   "metadata": {},
   "source": [
    "### **Retriever**\n",
    "- Búsqueda `vectores relevantes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f80b02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3}) # Le decimos que traiga los 3 chunks más relevantes según la pregunta realizada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62781886",
   "metadata": {},
   "source": [
    "### **Formateo de contenido**\n",
    "- Dividimos lo que sería la `fuente` y el `contenido` de la respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02c9d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs_with_sources(docs):\n",
    "    return \"\\n\\n---\\n\\n\".join(\n",
    "        f\"Fuente: {doc.metadata['source']}\\nFragmento: {doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7bff33",
   "metadata": {},
   "source": [
    "### **Prompt y su template**\n",
    "- Las reglas que seguirá nuestro modelo a la hora de responder y le decimos que recibirá un contexto y la pregunta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ee2af29",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Eres un \"Tutor de Investigación\" experto en IA. Tu única fuente de conocimiento son los siguientes fragmentos de papers fundacionales.\n",
    "\n",
    "REGLAS ESTRICTAS:\n",
    "1. Responde la pregunta del estudiante basándote *única y exclusivamente* en el contexto proporcionado.\n",
    "2. Al final de tu respuesta, DEBES citar la fuente exacta del paper que usaste (ej: \"Fuente: Attention Is All You Need (Vaswani et al., 2017)\").\n",
    "3. Si el contexto no contiene la información para responder, DEBES responder exactamente: \"Lo siento, no tengo información sobre eso en mis documentos fundacionales.\"\n",
    "\n",
    "---\n",
    "CONTEXTO PROPORCIONADO:\n",
    "{context}\n",
    "---\n",
    "\n",
    "PREGUNTA DEL ESTUDIANTE:\n",
    "{question}\n",
    "\n",
    "RESPUESTA DEL TUTOR:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a55f41",
   "metadata": {},
   "source": [
    "### **Conexión con el modelo**\n",
    "- El generador de respuesta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac7e4cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    api_key=api_key, # Utilizamos la api_key provista anteriormente.\n",
    "    model=\"gemini-2.5-flash\", # El modelo Gemini a utilizar.\n",
    "    temperature=0.3 # Baja temperatura para que sea fiel al texto.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c231dd",
   "metadata": {},
   "source": [
    "### **Pipeline completo**\n",
    "- Aqui `ensamblamos` todas las celdas creadas en ``Pipeline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5a736b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline creado\n"
     ]
    }
   ],
   "source": [
    "rag_chain = (\n",
    "    # Creamos un diccionario con el contexto y la pregunta\n",
    "    {\n",
    "        \"question\": RunnablePassthrough(), # La pregunta del usuario pasa tal cual.\n",
    "        \"context\": retriever | format_docs_with_sources # Llama a retriever y luego formatea los documentos.\n",
    "    }\n",
    "    \n",
    "    # Luego, pasamos ese diccionario al prompt\n",
    "    | prompt\n",
    "    \n",
    "    # Luego, pasamos el prompt al LLM\n",
    "    | llm\n",
    "    \n",
    "    # Finalmente, obtenemos la respuesta de texto\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"Pipeline creado\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe3a1e4",
   "metadata": {},
   "source": [
    "---\n",
    "## **Prueba**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5b7c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta 1: ¿Qué es un transformer?\n",
      "\n",
      "Respuesta del Tutor:\n",
      "El Transformer es una arquitectura de modelo que utiliza auto-atención apilada (stacked self-attention) y capas totalmente conectadas punto a punto (point-wise, fully connected layers) tanto para el codificador como para el decodificador.\n",
      "\n",
      "El codificador está compuesto por una pila de N capas idénticas (por ejemplo, N=6). Cada capa tiene dos sub-capas: la primera es un mecanismo de auto-atención multi-cabeza (multi-head self-attention), y la segunda es una red de alimentación hacia adelante (feed-forward network) simple y posicionalmente conectada. Se emplean conexiones residuales alrededor de cada una de las dos sub-capas, seguidas de normalización de capa.\n",
      "\n",
      "Una característica clave del Transformer es que permite una paralelización significativamente mayor. Además, reduce a un número constante de operaciones la cantidad necesaria para relacionar señales de dos posiciones de entrada o salida arbitrarias, lo que facilita el aprendizaje de dependencias entre posiciones distantes, a diferencia de modelos como ConvS2S (lineal) o ByteNet (logarítmico).\n",
      "\n",
      "Existen variaciones en la implementación del Transformer; por ejemplo, el Transformer de BERT utiliza auto-atención bidireccional, mientras que el Transformer de GPT usa auto-atención restringida donde cada token solo puede atender al contexto a su izquierda.\n",
      "\n",
      "Fuente: Attention Is All You Need (Vaswani et al., 2017); BERT: Pre-training of Deep Bidirectional Transformers (Devlin et al., 2018)\n"
     ]
    }
   ],
   "source": [
    "pregunta_1 = \"¿Qué es la 'atención' en un Transformer?\"\n",
    "print(f\"Pregunta 1: {pregunta_1}\\n\")\n",
    "\n",
    "print(\"Respuesta del Tutor:\")\n",
    "\n",
    "respuesta_1 = rag_chain.invoke(pregunta_1) # La pregunta del usuario ingresa al Pipeline.\n",
    "\n",
    "print(respuesta_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af751604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregunta 2: ¿Cómo funciona el mecanismo de recuperación en RAG?\n",
      "\n",
      "Respuesta del Tutor:\n",
      "El mecanismo de recuperación en RAG se diferencia de otros enfoques en que se enfoca en agregar contenido de varias piezas de contenido recuperado, en lugar de editar ligeramente un solo elemento recuperado. Además, RAG aprende la recuperación latente y recupera documentos de evidencia en lugar de pares de entrenamiento relacionados.\n",
      "\n",
      "Fuente: Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (Lewis et al., 2020)\n"
     ]
    }
   ],
   "source": [
    "pregunta_2 = \"¿Cómo funciona el mecanismo de recuperación en RAG?\"\n",
    "print(f\"Pregunta 2: {pregunta_2}\\n\")\n",
    "\n",
    "print(\"Respuesta del Tutor:\")\n",
    "\n",
    "respuesta_2 = rag_chain.invoke(pregunta_2) # La pregunta del usuario ingresa al Pipeline.\n",
    "\n",
    "print(respuesta_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
